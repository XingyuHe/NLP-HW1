{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from numpy.lib.function_base import vectorize\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import sparse\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, plot_confusion_matrix\n",
    "from better_profanity import profanity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading user  data \n",
    "USER_DATA = './resources/data/users.json'\n",
    "df_user = pd.read_json(USER_DATA, orient=\"index\")\n",
    "\n",
    "# loading training data .jsonl\n",
    "TRAINING_DATA = './resources/data/train.jsonl'\n",
    "VAL_DATA = './resources/data/val.jsonl'\n",
    "\n",
    "df_train, df_val = pd.read_json(TRAINING_DATA, lines=True), pd.read_json(VAL_DATA, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000000000000000000000000000000000000000000000000000000003',\n",
       " '0000000000000000001',\n",
       " '00000000001',\n",
       " '000000000064',\n",
       " '0000000001',\n",
       " '000000001',\n",
       " '000001',\n",
       " '00000a',\n",
       " '00001',\n",
       " '00001010',\n",
       " '00001585',\n",
       " '00002',\n",
       " '0000253187',\n",
       " '00006',\n",
       " '00007',\n",
       " '0001',\n",
       " '000154',\n",
       " '000164',\n",
       " '00017',\n",
       " '0003',\n",
       " '0005',\n",
       " '0006',\n",
       " '000ike',\n",
       " '000s',\n",
       " '000th',\n",
       " '000x',\n",
       " '000ï',\n",
       " '001',\n",
       " '00100000',\n",
       " '00100111',\n",
       " '00110000',\n",
       " '00110001',\n",
       " '00110100',\n",
       " '00110101',\n",
       " '00110110',\n",
       " '00111001',\n",
       " '00111011',\n",
       " '00111111',\n",
       " '001118',\n",
       " '0013k',\n",
       " '00142857143',\n",
       " '0016',\n",
       " '001th',\n",
       " '002',\n",
       " '0028',\n",
       " '00294',\n",
       " '003',\n",
       " '004',\n",
       " '005',\n",
       " '007',\n",
       " '009',\n",
       " '0095',\n",
       " '00am',\n",
       " '00pm',\n",
       " '01',\n",
       " '010',\n",
       " '01000110',\n",
       " '01001001',\n",
       " '011',\n",
       " '01100001',\n",
       " '01100010',\n",
       " '01100011',\n",
       " '01100100',\n",
       " '01100101',\n",
       " '01100110',\n",
       " '01100111',\n",
       " '01101000',\n",
       " '01101001',\n",
       " '01101010',\n",
       " '01101011',\n",
       " '01101100',\n",
       " '01101101',\n",
       " '01101110',\n",
       " '01101111',\n",
       " '01110000',\n",
       " '01110001',\n",
       " '01110010',\n",
       " '0111001001111001',\n",
       " '01110011',\n",
       " '01110100',\n",
       " '01110101',\n",
       " '01110110',\n",
       " '01110111',\n",
       " '01111000',\n",
       " '01111001',\n",
       " '01111010',\n",
       " '012',\n",
       " '014',\n",
       " '015',\n",
       " '016',\n",
       " '018',\n",
       " '01d',\n",
       " '02',\n",
       " '021',\n",
       " '0218181818',\n",
       " '024',\n",
       " '026',\n",
       " '026252144x',\n",
       " '028',\n",
       " '03',\n",
       " '030',\n",
       " '0312',\n",
       " '034',\n",
       " '0371',\n",
       " '038',\n",
       " '04',\n",
       " '04670a',\n",
       " '05',\n",
       " '051',\n",
       " '052',\n",
       " '05457148',\n",
       " '057',\n",
       " '05769a',\n",
       " '058',\n",
       " '059',\n",
       " '06',\n",
       " '060',\n",
       " '062',\n",
       " '0625',\n",
       " '0664221454',\n",
       " '067',\n",
       " '07',\n",
       " '070',\n",
       " '071',\n",
       " '07386a',\n",
       " '074',\n",
       " '076',\n",
       " '076207',\n",
       " '077',\n",
       " '078',\n",
       " '07h3r',\n",
       " '08',\n",
       " '08045a',\n",
       " '081',\n",
       " '0846',\n",
       " '08554',\n",
       " '0875',\n",
       " '0893',\n",
       " '09',\n",
       " '0900',\n",
       " '094',\n",
       " '095',\n",
       " '0_0',\n",
       " '0_o',\n",
       " '0b013e31811ed205',\n",
       " '0catch',\n",
       " '0ciqbebywcq',\n",
       " '0f',\n",
       " '0forfeit',\n",
       " '0in',\n",
       " '0m',\n",
       " '0mass',\n",
       " '0mph',\n",
       " '0pt',\n",
       " '0px',\n",
       " '0s',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000000',\n",
       " '10000000',\n",
       " '1000000000000',\n",
       " '100000s',\n",
       " '10001',\n",
       " '10008',\n",
       " '1000encyclopaedia',\n",
       " '1000mph',\n",
       " '1000s',\n",
       " '100106',\n",
       " '1002',\n",
       " '1007',\n",
       " '100c',\n",
       " '100k',\n",
       " '100mk',\n",
       " '100mph',\n",
       " '100s',\n",
       " '100th',\n",
       " '100x',\n",
       " '101',\n",
       " '10123',\n",
       " '1014',\n",
       " '1016',\n",
       " '1017',\n",
       " '102',\n",
       " '1020',\n",
       " '1022',\n",
       " '1024',\n",
       " '1025',\n",
       " '1025j',\n",
       " '1027',\n",
       " '1028',\n",
       " '103',\n",
       " '1030',\n",
       " '103032092393302039290443',\n",
       " '1031',\n",
       " '10320',\n",
       " '1033',\n",
       " '1034',\n",
       " '1035',\n",
       " '1036',\n",
       " '1038',\n",
       " '10390',\n",
       " '104',\n",
       " '1040',\n",
       " '1041',\n",
       " '10415',\n",
       " '1042',\n",
       " '10430',\n",
       " '1046',\n",
       " '1048',\n",
       " '104ad',\n",
       " '105',\n",
       " '1058',\n",
       " '106',\n",
       " '1060',\n",
       " '1066',\n",
       " '107',\n",
       " '1070',\n",
       " '10709',\n",
       " '10716',\n",
       " '107412',\n",
       " '108',\n",
       " '1080',\n",
       " '1084',\n",
       " '109',\n",
       " '1090',\n",
       " '10925',\n",
       " '1094',\n",
       " '1095',\n",
       " '1097',\n",
       " '1099',\n",
       " '10999',\n",
       " '10a',\n",
       " '10am',\n",
       " '10b',\n",
       " '10i',\n",
       " '10in',\n",
       " '10instinct',\n",
       " '10k',\n",
       " '10minutes',\n",
       " '10r36',\n",
       " '10s',\n",
       " '10she',\n",
       " '10th',\n",
       " '10ths',\n",
       " '10x',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '110th',\n",
       " '111',\n",
       " '1110',\n",
       " '1110ï',\n",
       " '1113',\n",
       " '112',\n",
       " '1124',\n",
       " '1126',\n",
       " '113',\n",
       " '1131',\n",
       " '1135',\n",
       " '1136',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '1160',\n",
       " '1164',\n",
       " '116billion',\n",
       " '117',\n",
       " '117393',\n",
       " '118',\n",
       " '1187',\n",
       " '118b',\n",
       " '119',\n",
       " '1191',\n",
       " '11919',\n",
       " '1192',\n",
       " '1195',\n",
       " '1197',\n",
       " '119a',\n",
       " '11and',\n",
       " '11commission',\n",
       " '11ft',\n",
       " '11points',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '1203',\n",
       " '1206',\n",
       " '1207',\n",
       " '1208',\n",
       " '1209',\n",
       " '121',\n",
       " '1211',\n",
       " '1212',\n",
       " '1213',\n",
       " '1214',\n",
       " '122',\n",
       " '1220',\n",
       " '1225',\n",
       " '1227',\n",
       " '123',\n",
       " '1232',\n",
       " '1234',\n",
       " '123456',\n",
       " '123exp',\n",
       " '123rf',\n",
       " '124',\n",
       " '1241',\n",
       " '125',\n",
       " '1250',\n",
       " '1258',\n",
       " '125safar',\n",
       " '126',\n",
       " '12631',\n",
       " '127',\n",
       " '1274',\n",
       " '128',\n",
       " '1280',\n",
       " '1285',\n",
       " '1289',\n",
       " '129',\n",
       " '1291',\n",
       " '12914014',\n",
       " '1296',\n",
       " '12965981',\n",
       " '12a',\n",
       " '12and',\n",
       " '12b',\n",
       " '12billion',\n",
       " '12i',\n",
       " '12km',\n",
       " '12milikan1',\n",
       " '12p',\n",
       " '12th',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '130years',\n",
       " '131',\n",
       " '1315',\n",
       " '1316',\n",
       " '132',\n",
       " '133',\n",
       " '13375',\n",
       " '133x',\n",
       " '134',\n",
       " '134a',\n",
       " '135',\n",
       " '136',\n",
       " '13635b',\n",
       " '1365',\n",
       " '1368',\n",
       " '137',\n",
       " '138',\n",
       " '138422374',\n",
       " '13865',\n",
       " '138888',\n",
       " '139',\n",
       " '1397',\n",
       " '13and',\n",
       " '13c',\n",
       " '13contractor',\n",
       " '13for',\n",
       " '13ft',\n",
       " '13k',\n",
       " '13millikan2',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400s',\n",
       " '140283',\n",
       " '141',\n",
       " '1415',\n",
       " '1418',\n",
       " '142',\n",
       " '1422',\n",
       " '143',\n",
       " '1436',\n",
       " '144',\n",
       " '1440',\n",
       " '1441',\n",
       " '1442',\n",
       " '145',\n",
       " '1452',\n",
       " '146',\n",
       " '1465',\n",
       " '147',\n",
       " '1470',\n",
       " '1471',\n",
       " '148',\n",
       " '1487',\n",
       " '149',\n",
       " '1490',\n",
       " '1492',\n",
       " '1495',\n",
       " '1498',\n",
       " '14a',\n",
       " '14and',\n",
       " '14for',\n",
       " '14k',\n",
       " '14m',\n",
       " '14th',\n",
       " '14wise',\n",
       " '14yr',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '150000000000',\n",
       " '15006b',\n",
       " '1500s',\n",
       " '1507',\n",
       " '150ad',\n",
       " '150billion',\n",
       " '150gt',\n",
       " '150kt',\n",
       " '150mph',\n",
       " '151',\n",
       " '1510',\n",
       " '1516',\n",
       " '1517',\n",
       " '1519',\n",
       " '152',\n",
       " '1520',\n",
       " '1525',\n",
       " '153',\n",
       " '1537',\n",
       " '1539',\n",
       " '154',\n",
       " '1543',\n",
       " '1546',\n",
       " '155',\n",
       " '156',\n",
       " '1562',\n",
       " '15625000000',\n",
       " '1563',\n",
       " '1566',\n",
       " '1568',\n",
       " '1569',\n",
       " '157',\n",
       " '1571',\n",
       " '158',\n",
       " '1587',\n",
       " '1589',\n",
       " '159',\n",
       " '1596',\n",
       " '15and',\n",
       " '15b',\n",
       " '15but',\n",
       " '15km',\n",
       " '15s',\n",
       " '15th',\n",
       " '15then',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600s',\n",
       " '160mph',\n",
       " '161',\n",
       " '16100',\n",
       " '1616',\n",
       " '162',\n",
       " '1620',\n",
       " '1620s',\n",
       " '16245',\n",
       " '1626',\n",
       " '1629',\n",
       " '163',\n",
       " '1631',\n",
       " '1634',\n",
       " '1635',\n",
       " '1637',\n",
       " '164',\n",
       " '1643',\n",
       " '1644',\n",
       " '165',\n",
       " '1650',\n",
       " '165812',\n",
       " '1659',\n",
       " '166',\n",
       " '1661',\n",
       " '1664',\n",
       " '1666666',\n",
       " '1667',\n",
       " '1668',\n",
       " '167',\n",
       " '1670',\n",
       " '168',\n",
       " '1685',\n",
       " '1689',\n",
       " '169',\n",
       " '1690',\n",
       " '16_fighting_falcon',\n",
       " '16a',\n",
       " '16gb',\n",
       " '16if',\n",
       " '16k',\n",
       " '16kadams',\n",
       " '16mm',\n",
       " '16th',\n",
       " '16this',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '17000',\n",
       " '1700s',\n",
       " '1703',\n",
       " '170c',\n",
       " '171',\n",
       " '172',\n",
       " '17296917',\n",
       " '173',\n",
       " '1731',\n",
       " '1738c',\n",
       " '174',\n",
       " '1740',\n",
       " '1740ï',\n",
       " '1744',\n",
       " '1745',\n",
       " '175',\n",
       " '1750',\n",
       " '1752',\n",
       " '1754',\n",
       " '1755',\n",
       " '1756',\n",
       " '17589370',\n",
       " '176',\n",
       " '1763',\n",
       " '1769',\n",
       " '177',\n",
       " '1770',\n",
       " '1775',\n",
       " '1776',\n",
       " '1777',\n",
       " '1778',\n",
       " '1779',\n",
       " '178',\n",
       " '1781',\n",
       " '1782',\n",
       " '1783',\n",
       " '1785',\n",
       " '1786',\n",
       " '1787',\n",
       " '1788',\n",
       " '1789',\n",
       " '179',\n",
       " '1790',\n",
       " '1791',\n",
       " '1793',\n",
       " '1794',\n",
       " '1795',\n",
       " '1796',\n",
       " '1797',\n",
       " '1799',\n",
       " '17all',\n",
       " '17and',\n",
       " '17for',\n",
       " '17forfeit',\n",
       " '17k',\n",
       " '17r36',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800s',\n",
       " '1800x',\n",
       " '1801',\n",
       " '1802',\n",
       " '1805',\n",
       " '1806',\n",
       " '1808',\n",
       " '1809',\n",
       " '180ad',\n",
       " '180ft',\n",
       " '180lbs',\n",
       " '180movie',\n",
       " '1810',\n",
       " '1812',\n",
       " '1813',\n",
       " '1814',\n",
       " '1815',\n",
       " '1818',\n",
       " '181st',\n",
       " '182',\n",
       " '1820',\n",
       " '1821',\n",
       " '1823',\n",
       " '1825',\n",
       " '183',\n",
       " '1830s',\n",
       " '1832',\n",
       " '1834',\n",
       " '1836',\n",
       " '1837',\n",
       " '184',\n",
       " '1840s',\n",
       " '1841',\n",
       " '1842',\n",
       " '1843',\n",
       " '1844',\n",
       " '1846',\n",
       " '1848',\n",
       " '1849',\n",
       " '185',\n",
       " '1850',\n",
       " '1852',\n",
       " '1854',\n",
       " '1855',\n",
       " '1856',\n",
       " '1859',\n",
       " '186',\n",
       " '1860',\n",
       " '186000',\n",
       " '1861',\n",
       " '1862',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '1866',\n",
       " '1867',\n",
       " '1868',\n",
       " '18688212',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '1873',\n",
       " '1875',\n",
       " '1876',\n",
       " '1877',\n",
       " '1878',\n",
       " '1879',\n",
       " '188',\n",
       " '1880',\n",
       " '1881',\n",
       " '1882',\n",
       " '1885',\n",
       " '1888',\n",
       " '1889',\n",
       " '189',\n",
       " '1890',\n",
       " '1890s',\n",
       " '1891',\n",
       " '1892',\n",
       " '1893',\n",
       " '1894',\n",
       " '1895',\n",
       " '1896',\n",
       " '1898',\n",
       " '1899',\n",
       " '18and',\n",
       " '18billion',\n",
       " '18but',\n",
       " '18in',\n",
       " '18s',\n",
       " '18th',\n",
       " '18they',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1901',\n",
       " '1903',\n",
       " '1904',\n",
       " '1905',\n",
       " '1906',\n",
       " '1907',\n",
       " '1909',\n",
       " '191',\n",
       " '1910',\n",
       " '1912',\n",
       " '1913',\n",
       " '1914',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '1919',\n",
       " '191yr',\n",
       " '192',\n",
       " '1920',\n",
       " '1920s',\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1931',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '194',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '195',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '196',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '197',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1989s',\n",
       " '199',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1991st',\n",
       " '1992',\n",
       " '1992ï',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1995a',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1998b',\n",
       " '1999',\n",
       " '19a',\n",
       " '19and',\n",
       " '19c',\n",
       " '19ff',\n",
       " '19now',\n",
       " '19so',\n",
       " '19th',\n",
       " '19x',\n",
       " '1a',\n",
       " '1anthropocentrism',\n",
       " '1archaic',\n",
       " '1ashwry',\n",
       " '1b',\n",
       " '1bc',\n",
       " '1billion',\n",
       " '1c',\n",
       " '1ce',\n",
       " '1cm',\n",
       " '1co',\n",
       " '1cr',\n",
       " '1d',\n",
       " '1dustpelt',\n",
       " '1e',\n",
       " '1everyone',\n",
       " '1f',\n",
       " '1g',\n",
       " '1herds',\n",
       " '1historygenius',\n",
       " '1historygeniusslapped',\n",
       " '1i',\n",
       " '1j',\n",
       " '1jn',\n",
       " '1john',\n",
       " '1john1',\n",
       " '1john3',\n",
       " '1k',\n",
       " '1kg',\n",
       " '1ki',\n",
       " '1kings',\n",
       " '1lb',\n",
       " '1life',\n",
       " '1m',\n",
       " '1moral',\n",
       " '1morï',\n",
       " '1mph',\n",
       " '1mya',\n",
       " '1n',\n",
       " '1noah',\n",
       " '1pe',\n",
       " '1peter',\n",
       " '1s',\n",
       " '1sa',\n",
       " '1st',\n",
       " '1stand',\n",
       " '1stlordofthevenerability',\n",
       " '1stlordofthevenerbility',\n",
       " '1stlordoftheveneribilty',\n",
       " '1ti',\n",
       " '1v1',\n",
       " '1x00',\n",
       " '1x10',\n",
       " '1z3d',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '2000bc',\n",
       " '2000s',\n",
       " '2000ï',\n",
       " '2001',\n",
       " '2001b',\n",
       " '2002',\n",
       " '2002_film',\n",
       " '2003',\n",
       " '2004',\n",
       " '2004a',\n",
       " '2004b',\n",
       " '2005',\n",
       " '2006',\n",
       " '20060',\n",
       " '2006b',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '200k',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2012_film',\n",
       " '2013',\n",
       " '20130912',\n",
       " '2014',\n",
       " '2014r32',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2020skills',\n",
       " '2021',\n",
       " '2022',\n",
       " '2023',\n",
       " '2025',\n",
       " '2027',\n",
       " '202bc',\n",
       " '203',\n",
       " '2030',\n",
       " '2032',\n",
       " '2035',\n",
       " '2038',\n",
       " '204',\n",
       " '2042',\n",
       " '2045',\n",
       " '2048',\n",
       " '205',\n",
       " '2050',\n",
       " '2052',\n",
       " '206',\n",
       " '2060',\n",
       " '207',\n",
       " '20767',\n",
       " '208',\n",
       " '209',\n",
       " '20a',\n",
       " '20and',\n",
       " '20birth',\n",
       " '20capital',\n",
       " '20dualism',\n",
       " '20feyerabend',\n",
       " '20idolatry',\n",
       " '20k',\n",
       " '20rate',\n",
       " '20s',\n",
       " '20t',\n",
       " '20th',\n",
       " '20will',\n",
       " '20x',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '2102',\n",
       " '2103',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '2134',\n",
       " '214',\n",
       " '215',\n",
       " '2150',\n",
       " '216',\n",
       " '2173',\n",
       " '218',\n",
       " '218350146',\n",
       " '219',\n",
       " '21and',\n",
       " '21envyings',\n",
       " '21st',\n",
       " '21trillion',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '22000',\n",
       " '2209',\n",
       " '221',\n",
       " '221k',\n",
       " '222',\n",
       " '223',\n",
       " '223368',\n",
       " '224',\n",
       " '225',\n",
       " '2250',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '22and',\n",
       " '22b',\n",
       " '22but',\n",
       " '22million',\n",
       " '22nd',\n",
       " '22ï',\n",
       " '23',\n",
       " '230',\n",
       " '2304',\n",
       " '2304bc',\n",
       " '2309',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '2340a',\n",
       " '23453km',\n",
       " '235',\n",
       " '2357',\n",
       " '2359',\n",
       " '236',\n",
       " '2360',\n",
       " '2364',\n",
       " '2369',\n",
       " '237',\n",
       " '2372',\n",
       " '238',\n",
       " '2388',\n",
       " '239',\n",
       " '2393',\n",
       " '23and',\n",
       " '23deaths',\n",
       " '23meekness',\n",
       " '23r36',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " ...]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Length\n",
    "# (b) Reference to the opponent\n",
    "# (c) Politeness words\n",
    "# (d) Swear words\n",
    "# (e) Personal pronouns\n",
    "# (f) Modal verbs\n",
    "# (g) Misspellings\n",
    "# (h) Links to outside websites\n",
    "# (i) Numbers\n",
    "# (j) Exclamation points\n",
    "# (k) Questions\n",
    "\n",
    "def get_length(document_side, vectorizer): \n",
    "    # Count the number if unigrams in a feature\n",
    "    document_pro = document_side[:, 0]\n",
    "    length_pro = np.sum(vectorizer(document_pro), axis=1)\n",
    "    \n",
    "    document_con = document_side[:, 1]\n",
    "    length_con = np.sum(vectorizer(document_con), axis=1)\n",
    "    \n",
    "    return length_pro, length_con\n",
    "\n",
    "def get_reference_to_opponent(df, document_side, vectorizer): \n",
    "    # Count the number of times the opponent's username is mentioned \n",
    "    pro_count = []\n",
    "    con_count = []\n",
    "    document_pro = document_side[:, 0]\n",
    "    document_con = document_side[:, 1]\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        opponent_name = df.loc[i, \"con_debator\"]\n",
    "        pro_count.append(document_pro.lower().count(opponent_name))\n",
    "        \n",
    "        opponent_name = df.loc[i, \"pro_debator\"]\n",
    "        con_count.append(document_con.lower().count(opponent_name))\n",
    "        \n",
    "    return np.array(pro_count), np.array(con_count) \n",
    "\n",
    "def get_politeness_words(document_side, vectorizer):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_swear_words(document_side, vectorizer):\n",
    "#     perhaps get rid some of the swear words because they look like they are necessary words \n",
    "#     for discussion such as arian, sodom \n",
    "    unigram = vectorizer.get_feature_names() \n",
    "    vector_swear = list(map(lambda x: int(profanity.contains_profanity(x)), unigram))\n",
    "    matrix_swear = np.reshape(vector_swear, newshape=[-1, 1])\n",
    "\n",
    "    document_pro = document_side[:, 0]\n",
    "    unigram_pro = vectorizer.transform(document_pro)\n",
    "    swear_pro = unigram_pro @ matrix_swear\n",
    "\n",
    "    document_con = document_side[:, 1]\n",
    "    unigram_con = vectorizer.transform(document_con)\n",
    "    swear_con = unigram_con @ matrix_swear\n",
    "\n",
    "def get_personal_pronouns(document_side, vectorizer):\n",
    "#     ================== faster but less accurate ========================\n",
    "    personal_pronouns = pd.Series([\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\"])\n",
    "\n",
    "    document_pro = document_side[:, 0]\n",
    "    document_con = document_side[:, 1]\n",
    "\n",
    "    all_counts_pro = []\n",
    "    all_counts_con = []\n",
    "\n",
    "    for name in personal_pronouns:\n",
    "        count_pro = np.array(list(map(lambda x: x.count(\" {} \".format(name)), document_pro)))\n",
    "        count_pro = np.reshape(count_pro, newshape=[-1, 1])\n",
    "        all_counts_pro.append(count_pro)\n",
    "\n",
    "        count_con = np.array(list(map(lambda x: x.count(\" {} \".format(name)), document_con)))\n",
    "        count_con = np.reshape(count_con, newshape=[-1, 1])\n",
    "        all_counts_con.append(count_con)\n",
    "\n",
    "    personal_pronouns_feature_pro = np.hstack(all_counts_pro)\n",
    "    personal_connouns_feature_con = np.hstack(all_counts_con)\n",
    "\n",
    "#     ================== more accurate but slower ========================\n",
    "    # personal_pronouns_vector = unigram_vectorizer.transform(personal_pronouns)\n",
    "    # matrix_person_pronouns = personal_pronouns_vector.T \n",
    "\n",
    "    # document_pro = document_side[:, 0]\n",
    "    # unigram_pro = vectorizer.transform(document_pro)\n",
    "    # personal_pronouns_feature_pro = unigram_pro @ matrix_person_pronouns\n",
    "    # I_count_pro = np.array(list(map(lambda x: x.count(\" I \"), document_pro)))\n",
    "    # I_count_pro = np.reshape(I_count_pro, newshape=[-1, 1])\n",
    "    # personal_pronouns_feature_pro = sparse.hstack([personal_pronouns_feature_pro, I_count_pro])\n",
    "\n",
    "    # document_con = document_side[:, 1]\n",
    "    # unigram_con = vectorizer.transform(document_con)\n",
    "    # personal_pronouns_feature_con = unigram_con @ matrix_person_pronouns\n",
    "    # I_count_con = np.array(list(map(lambda x: x.count(\" I \"), document_con)))\n",
    "    # I_count_con = np.reshape(I_count_con, newshape=[-1, 1])\n",
    "    # personal_pronouns_feature_con = sparse.hstack([personal_pronouns_feature_con, I_count_con])\n",
    "\n",
    "    return personal_pronouns_feature_pro, personal_pronouns_feature_con\n",
    "    \n",
    "        \n",
    "def get_questions(document_side, vectorizer):\n",
    "    \n",
    "    document_pro = document_side[:, 0]\n",
    "    question_count_pro = np.array(list(map(lambda x: x.count(\"?\"), document_pro)))\n",
    "\n",
    "    document_con = document_side[:, 1]\n",
    "    question_count_con = np.array(list(map(lambda x: x.count(\"?\"), document_con)))\n",
    "    \n",
    "    return question_count_pro, question_count_con\n",
    "    \n",
    "    \n",
    "def get_reference_website(document_side, vectorizer):\n",
    "    \n",
    "    document_pro = document_side[:, 0]\n",
    "    website_count_pro = np.array(list(map(lambda x: x.count(\"http\"), document_pro)))\n",
    "\n",
    "    document_con = document_side[:, 1]\n",
    "    website_count_con = np.array(list(map(lambda x: x.count(\"http\"), document_con)))\n",
    "    \n",
    "    return website_count_pro, website_count_con\n",
    "\n",
    "def get_exclamation(document_side, vectorizer):\n",
    "    \n",
    "    document_pro = document_side[:, 0]\n",
    "    exclamation_count_pro = np.array(list(map(lambda x: x.count(\"!\"), document_pro)))\n",
    "\n",
    "    document_con = document_side[:, 1]\n",
    "    exclamation_count_con = np.array(list(map(lambda x: x.count(\"!\"), document_con)))\n",
    "    \n",
    "    return exclamation_count_pro, exclamation_count_con\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126]\n",
      " [167]\n",
      " [126]\n",
      " ...\n",
      " [133]\n",
      " [  9]\n",
      " [ 59]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112 122 100 ...  83   8  55]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(personal_pronouns_feature_pro, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336 354 244 ... 199  19  74]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(personal_pronouns_feature_pro, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.400361061096191\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.150275230407715\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# document_side = document_train_side\n",
    "# vectorizer = unigram_vectorizer\n",
    "    \n",
    "# end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10in\n",
      "455\n",
      "717\n",
      "anal\n",
      "anus\n",
      "arian\n",
      "arse\n",
      "aryan\n",
      "asses\n",
      "b1tch\n",
      "b1tches\n",
      "ballsack\n",
      "bastard\n",
      "bdsm\n",
      "beastiality\n",
      "bestial\n",
      "bestiality\n",
      "bitch\n",
      "bitching\n",
      "blowjob\n",
      "bondage\n",
      "boobs\n",
      "breasts\n",
      "busty\n",
      "cipa\n",
      "clitoris\n",
      "cock\n",
      "crap\n",
      "crotch\n",
      "cum\n",
      "cunnilingus\n",
      "cunt\n",
      "damn\n",
      "dick\n",
      "dildo\n",
      "dildos\n",
      "dong\n",
      "douche\n",
      "douchebag\n",
      "drunk\n",
      "dummy\n",
      "dyke\n",
      "ejaculate\n",
      "ejaculating\n",
      "ejaculation\n",
      "enlargement\n",
      "erect\n",
      "erection\n",
      "erotic\n",
      "facial\n",
      "faggot\n",
      "fat\n",
      "fatass\n",
      "fellatio\n",
      "fisted\n",
      "fucked\n",
      "fuckin\n",
      "fuckwit\n",
      "fvck\n",
      "fvcked\n",
      "fvcker\n",
      "fvckin\n",
      "fvcking\n",
      "fvcks\n",
      "gal\n",
      "gay\n",
      "gays\n",
      "gigolo\n",
      "god\n",
      "god_\n",
      "goddamn\n",
      "goddamned\n",
      "gonads\n",
      "hell\n",
      "hemp\n",
      "heroin\n",
      "herpes\n",
      "hitler\n",
      "hiv\n",
      "homo\n",
      "homoerotic\n",
      "hookah\n",
      "hooker\n",
      "hore\n",
      "horny\n",
      "hump\n",
      "hymen\n",
      "inbred\n",
      "incest\n",
      "jackass\n",
      "jerk\n",
      "junkie\n",
      "junky\n",
      "kill\n",
      "kkk\n",
      "klan\n",
      "knob\n",
      "labia\n",
      "leper\n",
      "lesbians\n",
      "lmao\n",
      "loins\n",
      "lube\n",
      "lust\n",
      "lusting\n",
      "lusty\n",
      "masochist\n",
      "masterbation\n",
      "masturbate\n",
      "masturbating\n",
      "masturbation\n",
      "menstruate\n",
      "meth\n",
      "molest\n",
      "moron\n",
      "motherfucking\n",
      "muff\n",
      "murder\n",
      "nad\n",
      "naked\n",
      "nazi\n",
      "nazism\n",
      "negro\n",
      "nig\n",
      "nigga\n",
      "nimrod\n",
      "nipples\n",
      "nude\n",
      "nutsack\n",
      "omg\n",
      "opiate\n",
      "opium\n",
      "oral\n",
      "orally\n",
      "organ\n",
      "orgasm\n",
      "orgasmic\n",
      "orgasms\n",
      "orgies\n",
      "orgy\n",
      "ovary\n",
      "ovum\n",
      "p1ss\n",
      "paddy\n",
      "pantie\n",
      "panties\n",
      "pasty\n",
      "pawn\n",
      "pcp\n",
      "pecker\n",
      "pedophile\n",
      "pedophilia\n",
      "pee\n",
      "penetrate\n",
      "penetration\n",
      "penial\n",
      "penile\n",
      "penis\n",
      "perversion\n",
      "phallic\n",
      "pimp\n",
      "pinko\n",
      "piss\n",
      "pissed\n",
      "pisses\n",
      "pissing\n",
      "playboy\n",
      "poop\n",
      "porn\n",
      "porno\n",
      "pornography\n",
      "pot\n",
      "pr1ck\n",
      "prick\n",
      "pricks\n",
      "prostitute\n",
      "pubic\n",
      "punky\n",
      "puss\n",
      "pvssy\n",
      "queer\n",
      "quim\n",
      "rape\n",
      "raped\n",
      "raper\n",
      "raping\n",
      "rapist\n",
      "rectal\n",
      "rectum\n",
      "reich\n",
      "retard\n",
      "retarded\n",
      "rum\n",
      "schizo\n",
      "screw\n",
      "screwed\n",
      "screwing\n",
      "scrotum\n",
      "scum\n",
      "seaman\n",
      "seamen\n",
      "seduce\n",
      "semen\n",
      "sex\n",
      "sexual\n",
      "sh1t\n",
      "shite\n",
      "shitty\n",
      "shota\n",
      "skank\n",
      "slave\n",
      "sleaze\n",
      "sleazy\n",
      "slope\n",
      "sluts\n",
      "snatch\n",
      "sniper\n",
      "snuff\n",
      "sodom\n",
      "sperm\n",
      "spick\n",
      "spunk\n",
      "stoned\n",
      "strip\n",
      "stroke\n",
      "stupid\n",
      "suck\n",
      "sucked\n",
      "sucking\n",
      "t1ts\n",
      "tard\n",
      "teat\n",
      "testes\n",
      "testicle\n",
      "testis\n",
      "threesome\n",
      "thrust\n",
      "thug\n",
      "tit\n",
      "titties\n",
      "titty\n",
      "transsexual\n",
      "trashy\n",
      "turd\n",
      "ugly\n",
      "urine\n",
      "uterus\n",
      "vagina\n",
      "virgin\n",
      "vodka\n",
      "vomit\n",
      "vulgar\n",
      "wad\n",
      "wang\n",
      "wank\n",
      "weed\n",
      "weenie\n",
      "weiner\n",
      "weirdo\n",
      "wench\n",
      "wetback\n",
      "wh0re\n",
      "whiz\n",
      "whore\n",
      "whores\n",
      "whoring\n",
      "wigger\n",
      "willy\n",
      "womb\n",
      "woody\n",
      "wtf\n",
      "xx\n",
      "xxx\n",
      "½xx\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(vector_swear):\n",
    "    if val == 1:\n",
    "        print(unigram[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'I',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech = document_train_side[0][0]\n",
    "import re \n",
    "re.findall(\"I\", speech, flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonl(path):\n",
    "\n",
    "    with open(path) as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    data_list = []\n",
    "    for json_str in json_list:\n",
    "        data_list.append(json.loads(json_str))\n",
    "\n",
    "    return pd.DataFrame(data_list)\n",
    "def get_texts(df):\n",
    "    '''\n",
    "    Return a list of statements in df without differentiating the side of the speaker\n",
    "    '''\n",
    "\n",
    "    texts = []\n",
    "    for round in df.loc[:, 'rounds']:\n",
    "        for sub_round in round:\n",
    "            for speech in sub_round:\n",
    "                texts.append(speech['text'])\n",
    "\n",
    "    return texts\n",
    "\n",
    "def get_text_by_side(df): \n",
    "    '''\n",
    "    Return a list of documents where each document contains all text on one side in a \n",
    "    single debate\n",
    "    \n",
    "    text = [[Pro statement 1, Pro statement 2, ... Pro statement n],\n",
    "            [Con statement 1, Con statement 2, ... Con statement m]]\n",
    "            where n, m is the total number of statements from Pro and Con side across\n",
    "            all debates\n",
    "\n",
    "    size: [n x 2 x # statements in each debate]\n",
    "    '''\n",
    "\n",
    "    text = []\n",
    "    for round in df.loc[:, 'rounds']:\n",
    "        round_text = collections.defaultdict(list)\n",
    "\n",
    "        for sub_round in round:\n",
    "            for speech in sub_round: \n",
    "                round_text[speech['side']].append(speech['text'])\n",
    "\n",
    "        \n",
    "        text.append([\"\".join(round_text['Pro']), \"\".join(round_text['Con'])])\n",
    "\n",
    "    return np.array(text)\n",
    "\n",
    "def get_ngram_feature(document_side, vectorizer): \n",
    "    '''\n",
    "    Return the ngram features associated with a single debate\n",
    "\n",
    "    For pro side, each document is defined as a string that contains all the statements \n",
    "    from the pro side in a single debate (across different subrounds). Con side is \n",
    "    similarly defined. \n",
    "\n",
    "    return [[Pro side n gram vector, Con side n gram vector for 1 debate],\n",
    "            [Pro side n gram vector, Con side n gram vector for 2 debate],\n",
    "            ...]\n",
    "\n",
    "            size: [n, 2 x ngram count]\n",
    "    \n",
    "    Pro side and con side n gram vector are concatenated.\n",
    "    '''\n",
    "\n",
    "    pro_document = document_side[:, 0]\n",
    "    con_document = document_side[:, 1]\n",
    "\n",
    "    pro_feature = vectorizer.transform(pro_document)\n",
    "    con_feature = vectorizer.transform(con_document)\n",
    "    return sparse.hstack([pro_feature, con_feature])   \n",
    "\n",
    "def get_debate_feature(df):\n",
    "    '''\n",
    "    Return the debate feature such as category, pro_debator user name, etc\n",
    "\n",
    "    feature: [n, # of features] \n",
    "    '''\n",
    "    feature_name = ['category']\n",
    "    feature = []\n",
    "\n",
    "    for name in feature_name: \n",
    "        # TODO: check for data type of the column. If non-numeric, then do this\n",
    "        # otherwise, use the numerical data\n",
    "        encoding, unique_feature_val = pd.factorize(df[name])\n",
    "        feature.append(encoding)\n",
    "\n",
    "    return np.reshape(np.array(feature), [-1, len(feature_name)])\n",
    "\n",
    "def get_connotation_feature(document_side, matrix_connotation, vectorizer):\n",
    "    pro_document = document_side[:, 0]\n",
    "    con_document = document_side[:, 1]\n",
    "    \n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    \n",
    "    feature_pro = gram_pro @ matrix_connotation\n",
    "    feature_con = gram_con @ matrix_connotation\n",
    "    \n",
    "    return np.hstack([feature_pro, feature_con])\n",
    "\n",
    "def get_connotation_percentage_feature(document_side, matrix_connotation, vectorizer):\n",
    "    # create features where count of features are percentage points \n",
    "    pro_document = document_side[:, 0]\n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    feature_pro = gram_pro @ matrix_connotation\n",
    "    total_feature_count = np.reshape(np.sum(feature_pro, axis=1), newshape=(-1, 1))\n",
    "    feature_pct_pro = np.divide(feature_pro, total_feature_count)\n",
    "    feature_pct_pro[np.isneginf(feature_pct_pro)]=0\n",
    "    feature_pct_pro[np.isnan(feature_pct_pro)]=0\n",
    "    \n",
    "    con_document = document_side[:, 1]\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    feature_con = gram_con @ matrix_connotation\n",
    "    total_feature_count = np.reshape(np.sum(feature_con, axis=1), newshape=(-1, 1))\n",
    "    feature_pct_con = np.divide(feature_con, total_feature_count)\n",
    "    feature_pct_con[np.isneginf(feature_pct_con)]=0\n",
    "    feature_pct_con[np.isnan(feature_pct_con)]=0\n",
    "    \n",
    "    return np.hstack([feature_pct_pro, feature_pct_con])\n",
    "\n",
    "def get_connotation_ln_feature(document_side, matrix_connotation, vectorizer):\n",
    "    # create features where count of features are ln points \n",
    "    pro_document = document_side[:, 0]\n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    feature_pro = gram_pro @ matrix_connotation\n",
    "    feature_ln_pro = np.log(feature_pro)\n",
    "    feature_ln_pro[np.isneginf(feature_ln_pro)]=0\n",
    "    feature_ln_pro[np.isnan(feature_ln_pro)]=0\n",
    "    \n",
    "    con_document = document_side[:, 0]\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    feature_con = gram_con @ matrix_connotation\n",
    "    feature_ln_con = np.log(feature_con)\n",
    "    feature_ln_con[np.isneginf(feature_ln_con)]=0\n",
    "    feature_ln_con[np.isnan(feature_ln_con)]=0\n",
    "    \n",
    "    return np.hstack([feature_ln_pro, feature_ln_con])\n",
    "\n",
    "\n",
    "def get_vad_feature(document_side, matrix_vad, vectorizer):\n",
    "    pro_document = document_side[:, 0]\n",
    "    con_document = document_side[:, 1]\n",
    "    \n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    \n",
    "    feature_pro = gram_pro @ matrix_vad\n",
    "    feature_con = gram_con @ matrix_vad\n",
    "    \n",
    "    return np.hstack([feature_pro, feature_con])\n",
    "\n",
    "def get_vad_percentage_feature(document_side, matrix_vad, vectorizer):\n",
    "    # create features where count of features are percentage points \n",
    "    pro_document = document_side[:, 0]\n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    feature_pro = gram_pro @ matrix_vad\n",
    "    total_feature_count = np.reshape(np.sum(feature_pro, axis=1), newshape=(-1, 1))\n",
    "    feature_pct_pro = np.divide(feature_pro, total_feature_count)\n",
    "    feature_pct_pro[np.isneginf(feature_pct_pro)]=0\n",
    "    feature_pct_pro[np.isnan(feature_pct_pro)]=0\n",
    "    \n",
    "    con_document = document_side[:, 1]\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    feature_con = gram_con @ matrix_vad\n",
    "    total_feature_count = np.reshape(np.sum(feature_con, axis=1), newshape=(-1, 1))\n",
    "    feature_pct_con = np.divide(feature_con, total_feature_count)\n",
    "    feature_pct_con[np.isneginf(feature_pct_con)]=0\n",
    "    feature_pct_con[np.isnan(feature_pct_con)]=0\n",
    "    \n",
    "    return np.hstack([feature_pct_pro, feature_pct_con])\n",
    "\n",
    "def get_vad_ln_feature(document_side, matrix_vad, vectorizer):\n",
    "    # create features where count of features are ln points \n",
    "    pro_document = document_side[:, 0]\n",
    "    gram_pro = vectorizer.transform(pro_document)\n",
    "    feature_pro = gram_pro @ matrix_vad\n",
    "    feature_ln_pro = np.log(feature_pro)\n",
    "    feature_ln_pro[np.isneginf(feature_ln_pro)]=0\n",
    "    feature_ln_pro[np.isnan(feature_ln_pro)]=0\n",
    "    \n",
    "    con_document = document_side[:, 0]\n",
    "    gram_con = vectorizer.transform(con_document)\n",
    "    feature_con = gram_con @ matrix_vad\n",
    "    feature_ln_con = np.log(feature_con)\n",
    "    feature_ln_con[np.isneginf(feature_ln_con)]=0\n",
    "    feature_ln_con[np.isnan(feature_ln_con)]=0\n",
    "    \n",
    "    return np.hstack([feature_ln_pro, feature_ln_con])\n",
    "\n",
    "def get_winner(df): \n",
    "    '''\n",
    "    Cons gets mapped to 0 and pro gets mapped to 1\n",
    "    '''\n",
    "    return df.loc[:, \"winner\"].replace({\"Con\": 0, \"Pro\": 1})\n",
    "\n",
    "def get_all_feature_label(df, vectorizer):\n",
    "    '''\n",
    "    Return the training input and validation input that contains all features, \n",
    "    which are ngram features and debate features\n",
    "    '''\n",
    "    \n",
    "    # Getting two sets of features - ngram and debate related features\n",
    "    ngram_feature = get_ngram_feature(df, vectorizer)\n",
    "\n",
    "    # debate_feature = get_debate_feature(df)\n",
    "\n",
    "    # Combining two sets of features\n",
    "    # X = sparse.hstack([debate_feature, ngram_feature])\n",
    "    X = sparse.hstack([ngram_feature])\n",
    "\n",
    "    y = np.array(get_winner(df))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - lex feature, debate feature, n-gram feature\n",
    "This model should use\n",
    "1. word ngrams\n",
    "2. lexicon based features: implement lexicon based features for a lexicon of your choice\n",
    "   1. Connotation lexicon\n",
    "   2. NRC-VAD lexicon\n",
    "   3. How you extract features is part of the desgin decision that you need to make. One simple example for lexical features could be counting how many words in each debaters language appear in the corresponding lexicon. \n",
    "\n",
    "TODO: \n",
    "1. Read connotation - 1 file\n",
    "2. NRC features - 2 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xhe/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/ipykernel_launcher.py:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# 1. Read connotation - 1 file\n",
    "# 2. NRC features - 2 files \n",
    "CONNOTATION = \"./resources/lexica/connotation_lexicon_a.0.1.csv\"\n",
    "NRC_LEXICON_VAD = \"./resources/lexica/NRC-VAD-Lexicon-Aug2018Release/NRC-VAD-Lexicon.txt\"\n",
    "NRC_LEXICON_SORTED_VALENCE = \"./resources/lexica/NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/v-scores.txt\"\n",
    "NRC_LEXICON_SORTED_AROUSAL = \"./resources/lexica/NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/a-scores.txt\"\n",
    "NRC_LEXICON_SORTED_DOMINANCE = \"./resources/lexica/NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/d-scores.txt\"\n",
    "\n",
    "df_connotation = pd.read_csv(CONNOTATION, sep=\",|_\", header=None)\n",
    "df_connotation.columns = [\"word\", \"pos\", \"connotation\"] # word, part of speech, connotation\n",
    "df_connotation = df_connotation.dropna() # There are five words in the connotation that are nan \n",
    "df_connotation = df_connotation.set_index(\"word\")\n",
    "df_connotation[\"pos\"] = df_connotation[\"pos\"].astype('category')\n",
    "df_connotation = df_connotation.drop(columns=[\"pos\"]) # drop the part of speech classification because we can't use it now \n",
    "df_connotation[\"connotation\"] = df_connotation[\"connotation\"].astype('category')\n",
    "df_connotation = pd.get_dummies(df_connotation)\n",
    "\n",
    "df_nrc_vad = pd.read_csv(NRC_LEXICON_VAD, sep=\"\t\", header=None)\n",
    "df_nrc_vad.columns = [\"word\", \"valence\", \"arousal\", \"dominance\"]\n",
    "df_nrc_vad = df_nrc_vad.dropna()\n",
    "df_nrc_vad = df_nrc_vad.set_index(\"word\")\n",
    "df_nrc_vad[\"valence\"] = df_nrc_vad[\"valence\"].astype('category')\n",
    "df_nrc_vad[\"arousal\"] = df_nrc_vad[\"arousal\"].astype('category')\n",
    "df_nrc_vad[\"dominance\"] = df_nrc_vad[\"dominance\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.8, min_df=0.2, ngram_range=(1, 3),\n",
       "                stop_words='english', sublinear_tf=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get features and labels for traininig and validation \n",
    "unigram_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# Generate the corpus for vectotrizer to fit on \n",
    "document_train_side = get_text_by_side(df_train)\n",
    "document_val_side = get_text_by_side(df_val)\n",
    "document_train = [side[0] + side[1] for side in document_train_side]\n",
    "document_val = [side[0] + side[1] for side in document_val_side]\n",
    "\n",
    "# The vectorizer trains all all the textual corpus regardless of the side \n",
    "# of the debate \n",
    "unigram_vectorizer.fit(document_train)\n",
    "\n",
    "# Get the feature vector of a sentence using ngram @ matrix_connotation\n",
    "# Creating the matrix \n",
    "word_connotation = df_connotation.index\n",
    "word_vector_connotation = unigram_vectorizer.transform(word_connotation)\n",
    "matrix_connotation = word_vector_connotation.T @ df_connotation\n",
    "matrix_connotation_no_neutral = word_vector_connotation.T @ df_connotation.drop(columns=[\"connotation_neutral\"])\n",
    "\n",
    "word_vad = df_nrc_vad.index\n",
    "word_vector_vad = unigram_vectorizer.transform(word_vad)\n",
    "matrix_vad = word_vector_vad.T @ df_nrc_vad\n",
    "# For words with mulitple part of speech, we are counting the total\n",
    "# sum across all part of speech of that word for each feature \n",
    "\n",
    "# Get label \n",
    "label_train = get_winner(df_train)\n",
    "label_val = get_winner(df_val)\n",
    "\n",
    "y_train = np.array(label_train)\n",
    "y_val = np.array(label_val)\n",
    "\n",
    "# Get more grams \n",
    "trigram_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=0.2, stop_words='english', ngram_range=(1,3))\n",
    "trigram_vectorizer.fit(document_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================== you can run experiments here ======================\n",
    "# Get all TRAINING features:\n",
    "# Get the documents on pro and con side so that we can forming feature \n",
    "# vector on both sides for training \n",
    "\n",
    "trigram_train = get_ngram_feature(document_side=document_train_side, vectorizer=trigram_vectorizer)\n",
    "# ============= using raw number counts of the feature ==========================\n",
    "# feature_connotation_train = get_connotation_feature(document_side=document_train_side,\n",
    "#                                                         matrix_connotation=matrix_connotation,\n",
    "#                                                         vectorizer=unigram_vectorizer)\n",
    "# feature_vad_train = get_vad_feature(document_side=document_train_side,\n",
    "#                                                         matrix_vad=matrix_vad,\n",
    "#                                                         vectorizer=unigram_vectorizer)\n",
    "# feature_train = sparse.hstack([trigram_train, feature_connotation_train, \n",
    "#                     feature_vad_train])\n",
    "\n",
    "# ============= using percentage counts of the feature ==========================\n",
    "# feature_connotation_pct_train = get_connotation_percentage_feature(document_train_side, matrix_connotation, unigram_vectorizer)\n",
    "# feature_vad_pct_train = get_vad_percentage_feature(document_train_side, matrix_vad, unigram_vectorizer)\n",
    "# feature_train = sparse.hstack([trigram_train, feature_connotation_pct_train, \n",
    "#                     feature_vad_pct_train])\n",
    "\n",
    "# ============= using log counts of the feature ==========================\n",
    "# feature_connotation_ln_train = get_connotation_ln_feature(document_train_side, matrix_connotation, unigram_vectorizer)\n",
    "# feature_vad_ln_train = get_vad_ln_feature(document_train_side, matrix_vad, unigram_vectorizer)\n",
    "# feature_train = sparse.hstack([trigram_train, feature_connotation_ln_train, \n",
    "#                     feature_vad_ln_train])\n",
    "\n",
    "# ============= using percentage counts of the feature without neutral connotation ==========================\n",
    "feature_connotation_pct_train = get_connotation_percentage_feature(document_train_side, matrix_connotation_no_neutral, unigram_vectorizer)\n",
    "feature_vad_pct_train = get_vad_percentage_feature(document_train_side, matrix_vad, unigram_vectorizer)\n",
    "feature_train = sparse.hstack([trigram_train, feature_connotation_pct_train, \n",
    "                    feature_vad_pct_train])\n",
    "\n",
    "# Get all VALIDATION features:\n",
    "trigram_val = get_ngram_feature(document_side=document_val_side, vectorizer=trigram_vectorizer)\n",
    "# ============= using raw counts of of the feature ==========================\n",
    "# feature_connotation_val = get_connotation_feature(document_side=document_val_side,\n",
    "#                                                         matrix_connotation=matrix_connotation,\n",
    "#                                                         vectorizer=unigram_vectorizer)\n",
    "# feature_vad_val = get_vad_feature(document_side=document_val_side,\n",
    "#                                                         matrix_vad=matrix_vad,\n",
    "#                                                         vectorizer=unigram_vectorizer)\n",
    "# feature_val = sparse.hstack([trigram_val, feature_connotation_val, \n",
    "#                     feature_vad_val])\n",
    "\n",
    "# ============= using percentage count of of the feature ==========================\n",
    "# feature_connotation_pct_val = get_connotation_percentage_feature(document_val_side, matrix_connotation, unigram_vectorizer)\n",
    "# feature_vad_pct_val = get_vad_percentage_feature(document_val_side, matrix_vad, unigram_vectorizer)\n",
    "# feature_train = sparse.hstack([trigram_train, feature_connotation_pct_train, \n",
    "#                     feature_vad_pct_train])\n",
    "# feature_val = sparse.hstack([trigram_val, feature_connotation_pct_val, \n",
    "#                     feature_vad_pct_val])\n",
    "\n",
    "# ============= using log counts of the feature ==========================\n",
    "# feature_connotation_ln_val = get_connotation_ln_feature(document_val_side, matrix_connotation, unigram_vectorizer)\n",
    "# feature_vad_ln_val = get_vad_ln_feature(document_val_side, matrix_vad, unigram_vectorizer)\n",
    "\n",
    "# feature_val = sparse.hstack([trigram_val, feature_connotation_ln_val, \n",
    "#                     feature_vad_ln_val])\n",
    "\n",
    "# ============= using percentage counts of the feature without neutral connotation ==========================\n",
    "feature_connotation_pct_val = get_connotation_percentage_feature(document_val_side, matrix_connotation_no_neutral, unigram_vectorizer)\n",
    "feature_vad_pct_val = get_vad_percentage_feature(document_val_side, matrix_vad, unigram_vectorizer)\n",
    "\n",
    "feature_val = sparse.hstack([trigram_val, feature_connotation_pct_val, \n",
    "                    feature_vad_pct_val])\n",
    "\n",
    "# Create model\n",
    "clf = sklearn.linear_model.LogisticRegression()\n",
    "clf.fit(feature_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       916\n",
      "           1       0.91      0.82      0.86       676\n",
      "\n",
      "    accuracy                           0.89      1592\n",
      "   macro avg       0.89      0.88      0.88      1592\n",
      "weighted avg       0.89      0.89      0.89      1592\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.77       211\n",
      "           1       0.77      0.60      0.68       188\n",
      "\n",
      "    accuracy                           0.73       399\n",
      "   macro avg       0.74      0.72      0.72       399\n",
      "weighted avg       0.74      0.73      0.72       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, clf.predict(feature_train)))\n",
    "print(classification_report(y_val, clf.predict(feature_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Here is the model that only uses debate features and ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro , con shape are (1592, 606) (1592, 606)\n",
      "pro , con shape are (399, 606) (399, 606)\n"
     ]
    }
   ],
   "source": [
    "# Extracting texts from training and testing data\n",
    "label_train = get_winner(df_train)\n",
    "label_val = get_winner(df_val)\n",
    "\n",
    "# Generate the corpus \n",
    "document_train = get_text_by_side(df_train)\n",
    "document_val = get_text_by_side(df_val)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.9, min_df=0.1, stop_words='english', ngram_range=(1,3))\n",
    "vectorizer.fit(document_train)\n",
    "\n",
    "# Getting two sets of features - ngram and debate related features\n",
    "ngram_feature_train = get_ngram_feature(df_train, vectorizer)\n",
    "ngram_feature_val = get_ngram_feature(df_val, vectorizer)\n",
    "\n",
    "debate_feature_train = get_debate_feature(df_train)\n",
    "debate_feture_val = get_debate_feature(df_val)\n",
    "\n",
    "# Combining two sets of features\n",
    "X_train = sparse.hstack([debate_feature_train, ngram_feature_train])\n",
    "X_val = sparse.hstack([debate_feture_val, ngram_feature_val])\n",
    "\n",
    "y_train = np.array(label_train)\n",
    "y_val = np.array(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check\n",
      "1592 number of observations in the training set\n",
      "(1592, 1213) number of observation x the size of ngram vectors in the training set\n",
      "(1592,) number of labels in the training set\n",
      "399 number of observations in the validation set\n",
      "(399, 1213) number of observation x the size of ngram vectors in the validation set\n",
      "(399,) number of labels in the validation set\n"
     ]
    }
   ],
   "source": [
    "print('Sanity check')\n",
    "print(df_train.shape[0], 'number of observations in the training set')\n",
    "print(X_train.shape, 'number of observation x the size of ngram vectors in the training set')\n",
    "print(y_train.shape, 'number of labels in the training set')\n",
    "print(df_val.shape[0], 'number of observations in the validation set')\n",
    "print(X_val.shape, 'number of observation x the size of ngram vectors in the validation set')\n",
    "print(y_val.shape, 'number of labels in the validation set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training set report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.88      0.94      0.91       916\n",
      "         Con       0.91      0.83      0.87       676\n",
      "\n",
      "    accuracy                           0.89      1592\n",
      "   macro avg       0.90      0.88      0.89      1592\n",
      "weighted avg       0.89      0.89      0.89      1592\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.72      0.86      0.78       211\n",
      "         Con       0.80      0.62      0.70       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.76      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building and training the model\n",
    "clf = sklearn.linear_model.LogisticRegression()\n",
    "clf.fit(ngram_feature_train, y_train)\n",
    "\n",
    "print(\"Logistic Regression training set report:\")\n",
    "print(classification_report(y_train, clf.predict(ngram_feature_train), target_names=['Pro', 'Con']))\n",
    "print(classification_report(y_val, clf.predict(ngram_feature_val), target_names=['Pro', 'Con']))\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<399x1213 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 96112 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1682716 features per sample; expecting 1213",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-aa2b8b9a62ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluating the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_religion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression testing set report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Con'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 289\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1682716 features per sample; expecting 1213"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the validation set\n",
    "y_predicted = clf.predict(X_val_religion)\n",
    "print(\"Logistic Regression testing set report:\")\n",
    "print(classification_report(y_val_religion, y_predicted, target_names=['Pro', 'Con']))\n",
    "\n",
    "print(\"Accuracy score: \",accuracy_score(y_val_religion, y_predicted))\n",
    "print(\"Balanced accuracy score: \",accuracy_score(y_val_religion, y_predicted))\n",
    "\n",
    "plot_confusion_matrix(clf, X_val, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning ngram models over max_df and min_df\n",
    "def search_max_df_min_df(df_train, df_val):\n",
    "    highest_acc, best_min_df, best_max_df = 0, -1, -1\n",
    "    report = {}\n",
    "    for min_df in np.arange(0, 1, 0.1):\n",
    "        for diff in np.arange(0.1, 1 - min_df, 0.1):\n",
    "            max_df = min_df + diff\n",
    "\n",
    "            vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=max_df, min_df=min_df, stop_words='english', ngram_range=(1,3))\n",
    "            document_train = get_text_by_side(df_train)\n",
    "            vectorizer.fit(document_train)\n",
    "            X_train, y_train = get_all_feature_label(df_train, vectorizer)\n",
    "            X_val, y_val = get_all_feature_label(df_val, vectorizer)\n",
    "\n",
    "            clf = sklearn.linear_model.LogisticRegression()\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            print(\"====================================\")\n",
    "\n",
    "            y_predicted = clf.predict(X_val)\n",
    "            print(\"Logistic Regression testing set report:\")\n",
    "            report[(min_df, max_df)] = classification_report(y_val, y_predicted, target_names=['Pro', 'Con'], output_dict=True)\n",
    "            acc = accuracy_score(y_val, y_predicted)\n",
    "\n",
    "            print(\"max_df: {}, min_df: {}, accuracy: {}\".format(max_df, min_df, acc))\n",
    "\n",
    "            if acc > highest_acc:\n",
    "                highest_acc, best_min_df, best_max_df = acc, min_df, max_df\n",
    "\n",
    "    print(\"************ best min_df, best max_df, acc\", best_min_df, best_max_df, highest_acc)\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "(0.0, 0.1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.54      1.00      0.70       211\n",
      "         Con       1.00      0.06      0.11       188\n",
      "\n",
      "    accuracy                           0.56       399\n",
      "   macro avg       0.77      0.53      0.41       399\n",
      "weighted avg       0.76      0.56      0.42       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.57      1.00      0.72       211\n",
      "         Con       1.00      0.14      0.25       188\n",
      "\n",
      "    accuracy                           0.60       399\n",
      "   macro avg       0.78      0.57      0.49       399\n",
      "weighted avg       0.77      0.60      0.50       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.30000000000000004)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.57      1.00      0.73       211\n",
      "         Con       1.00      0.15      0.26       188\n",
      "\n",
      "    accuracy                           0.60       399\n",
      "   macro avg       0.78      0.57      0.49       399\n",
      "weighted avg       0.77      0.60      0.51       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.57      1.00      0.73       211\n",
      "         Con       1.00      0.17      0.29       188\n",
      "\n",
      "    accuracy                           0.61       399\n",
      "   macro avg       0.79      0.59      0.51       399\n",
      "weighted avg       0.78      0.61      0.52       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.58      1.00      0.73       211\n",
      "         Con       1.00      0.18      0.31       188\n",
      "\n",
      "    accuracy                           0.61       399\n",
      "   macro avg       0.79      0.59      0.52       399\n",
      "weighted avg       0.78      0.61      0.53       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.58      1.00      0.74       211\n",
      "         Con       1.00      0.20      0.34       188\n",
      "\n",
      "    accuracy                           0.62       399\n",
      "   macro avg       0.79      0.60      0.54       399\n",
      "weighted avg       0.78      0.62      0.55       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.7000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.59      1.00      0.74       211\n",
      "         Con       1.00      0.21      0.34       188\n",
      "\n",
      "    accuracy                           0.63       399\n",
      "   macro avg       0.79      0.60      0.54       399\n",
      "weighted avg       0.78      0.63      0.55       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.59      1.00      0.74       211\n",
      "         Con       1.00      0.21      0.35       188\n",
      "\n",
      "    accuracy                           0.63       399\n",
      "   macro avg       0.79      0.61      0.55       399\n",
      "weighted avg       0.78      0.63      0.56       399\n",
      "\n",
      "====================\n",
      "(0.0, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.59      1.00      0.74       211\n",
      "         Con       1.00      0.21      0.35       188\n",
      "\n",
      "    accuracy                           0.63       399\n",
      "   macro avg       0.79      0.61      0.55       399\n",
      "weighted avg       0.78      0.63      0.56       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.88      0.78       211\n",
      "         Con       0.82      0.59      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.74      0.73       399\n",
      "weighted avg       0.76      0.74      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.30000000000000004)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.90      0.79       211\n",
      "         Con       0.83      0.58      0.68       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.70      0.90      0.79       211\n",
      "         Con       0.84      0.57      0.68       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.77      0.73      0.73       399\n",
      "weighted avg       0.76      0.74      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.69      0.90      0.78       211\n",
      "         Con       0.83      0.56      0.67       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.73      0.72       399\n",
      "weighted avg       0.76      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.90      0.79       211\n",
      "         Con       0.83      0.58      0.68       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.7)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.90      0.79       211\n",
      "         Con       0.84      0.59      0.69       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.77      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.89      0.79       211\n",
      "         Con       0.82      0.59      0.69       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.1, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.90      0.79       211\n",
      "         Con       0.83      0.59      0.69       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.77      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.30000000000000004)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.86      0.78       211\n",
      "         Con       0.80      0.61      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.74      0.74       399\n",
      "weighted avg       0.75      0.74      0.74       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.70      0.87      0.78       211\n",
      "         Con       0.80      0.59      0.68       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.75      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.70      0.88      0.78       211\n",
      "         Con       0.81      0.57      0.67       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.73      0.73       399\n",
      "weighted avg       0.75      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.6000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.72      0.88      0.79       211\n",
      "         Con       0.82      0.61      0.70       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.75      0.75       399\n",
      "weighted avg       0.77      0.75      0.75       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.7)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.89      0.79       211\n",
      "         Con       0.82      0.59      0.68       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.74      0.73       399\n",
      "weighted avg       0.76      0.74      0.74       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.72      0.90      0.80       211\n",
      "         Con       0.84      0.60      0.70       188\n",
      "\n",
      "    accuracy                           0.76       399\n",
      "   macro avg       0.78      0.75      0.75       399\n",
      "weighted avg       0.77      0.76      0.75       399\n",
      "\n",
      "====================\n",
      "(0.2, 0.9000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.90      0.79       211\n",
      "         Con       0.84      0.60      0.70       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.75      0.74       399\n",
      "weighted avg       0.77      0.75      0.75       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.4)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.68      0.84      0.75       211\n",
      "         Con       0.76      0.56      0.65       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.72      0.70      0.70       399\n",
      "weighted avg       0.72      0.71      0.70       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.69      0.85      0.76       211\n",
      "         Con       0.77      0.58      0.66       188\n",
      "\n",
      "    accuracy                           0.72       399\n",
      "   macro avg       0.73      0.71      0.71       399\n",
      "weighted avg       0.73      0.72      0.72       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.6000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.88      0.79       211\n",
      "         Con       0.82      0.60      0.69       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.77      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.7000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.87      0.78       211\n",
      "         Con       0.81      0.60      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.76      0.74      0.74       399\n",
      "weighted avg       0.76      0.74      0.74       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.85      0.77       211\n",
      "         Con       0.78      0.61      0.68       188\n",
      "\n",
      "    accuracy                           0.73       399\n",
      "   macro avg       0.74      0.73      0.73       399\n",
      "weighted avg       0.74      0.73      0.73       399\n",
      "\n",
      "====================\n",
      "(0.30000000000000004, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.72      0.87      0.79       211\n",
      "         Con       0.81      0.62      0.70       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.76      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.75       399\n",
      "\n",
      "====================\n",
      "(0.4, 0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.69      0.82      0.75       211\n",
      "         Con       0.74      0.59      0.65       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.72      0.70      0.70       399\n",
      "weighted avg       0.71      0.71      0.70       399\n",
      "\n",
      "====================\n",
      "(0.4, 0.6000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.72      0.86      0.78       211\n",
      "         Con       0.80      0.62      0.70       188\n",
      "\n",
      "    accuracy                           0.75       399\n",
      "   macro avg       0.76      0.74      0.74       399\n",
      "weighted avg       0.76      0.75      0.74       399\n",
      "\n",
      "====================\n",
      "(0.4, 0.7000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.85      0.77       211\n",
      "         Con       0.78      0.61      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.74      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.4, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.85      0.77       211\n",
      "         Con       0.79      0.61      0.68       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.75      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.4, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.86      0.78       211\n",
      "         Con       0.80      0.60      0.68       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.75      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.5, 0.6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.84      0.77       211\n",
      "         Con       0.78      0.62      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.74      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.5, 0.7)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.70      0.86      0.77       211\n",
      "         Con       0.79      0.60      0.68       188\n",
      "\n",
      "    accuracy                           0.73       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.74      0.73      0.73       399\n",
      "\n",
      "====================\n",
      "(0.5, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.85      0.77       211\n",
      "         Con       0.78      0.60      0.68       188\n",
      "\n",
      "    accuracy                           0.73       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.74      0.73      0.73       399\n",
      "\n",
      "====================\n",
      "(0.5, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.71      0.86      0.78       211\n",
      "         Con       0.79      0.61      0.69       188\n",
      "\n",
      "    accuracy                           0.74       399\n",
      "   macro avg       0.75      0.73      0.73       399\n",
      "weighted avg       0.75      0.74      0.73       399\n",
      "\n",
      "====================\n",
      "(0.6000000000000001, 0.7000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.67      0.85      0.75       211\n",
      "         Con       0.76      0.53      0.62       188\n",
      "\n",
      "    accuracy                           0.70       399\n",
      "   macro avg       0.72      0.69      0.69       399\n",
      "weighted avg       0.71      0.70      0.69       399\n",
      "\n",
      "====================\n",
      "(0.6000000000000001, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.67      0.88      0.76       211\n",
      "         Con       0.79      0.52      0.62       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.73      0.70      0.69       399\n",
      "weighted avg       0.73      0.71      0.70       399\n",
      "\n",
      "====================\n",
      "(0.6000000000000001, 0.9000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.67      0.88      0.76       211\n",
      "         Con       0.79      0.51      0.62       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.73      0.70      0.69       399\n",
      "weighted avg       0.73      0.71      0.70       399\n",
      "\n",
      "====================\n",
      "(0.7000000000000001, 0.8)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.67      0.92      0.77       211\n",
      "         Con       0.84      0.48      0.61       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.75      0.70      0.69       399\n",
      "weighted avg       0.75      0.71      0.70       399\n",
      "\n",
      "====================\n",
      "(0.7000000000000001, 0.9000000000000001)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.66      0.91      0.77       211\n",
      "         Con       0.83      0.48      0.61       188\n",
      "\n",
      "    accuracy                           0.71       399\n",
      "   macro avg       0.75      0.70      0.69       399\n",
      "weighted avg       0.74      0.71      0.69       399\n",
      "\n",
      "====================\n",
      "(0.8, 0.9)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pro       0.61      0.91      0.73       211\n",
      "         Con       0.77      0.34      0.47       188\n",
      "\n",
      "    accuracy                           0.64       399\n",
      "   macro avg       0.69      0.62      0.60       399\n",
      "weighted avg       0.68      0.64      0.60       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "max_acc, best_min_df, best_max_df = 0, -1, -1\n",
    "gram3_report = report\n",
    "\n",
    "for key, val in report.items():\n",
    "    print(\"====================\")\n",
    "    print(key)\n",
    "    print(val)\n",
    "\n",
    "# The best min df and the best max df are (0.2, 0.8) with validation accuracy of 0.76\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of achieving this is to create two n-gram models. One n-gram model outputs features\n",
    "for religious topics and another n-gram model outputs features for non-religious topics.\n",
    "By limiting the corpus within their topics, the Tf_idf scores may better reflect the \n",
    "proper weighting. For example, certain words that might only appear in winning relgious debates\n",
    "but also appear in all other losing debates may now have a significantly different score from \n",
    "words that appear in only losing religous debates but appear in all other winning debates. \n",
    "Previously, these two sets of words would have similar tf_idf score but are not helpful \n",
    "towards predicting winning debates because their prediciton power within relgious topic is\n",
    "diluted by the non-religous topics. By limiting the corpus scope, we can see that these \n",
    "words become helpful in both religous and non-relgious debates.\n",
    "\n",
    "TODO:\n",
    "1. Define a Tfidfvectorizer for both religous and non-religious topics\n",
    "2. Train the vectorizer using their respective subsets\n",
    "3. Depending the topic of the new data, we should use the two models conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data sets\n",
    "df_train_religion = df_train.loc[df_train.category == \"Religion\" ,:]\n",
    "df_train_other = df_train.loc[df_train.category != \"Religion\" ,:]\n",
    "df_val_religion = df_val.loc[df_val.category == \"Religion\" ,:]\n",
    "df_val_other = df_val.loc[df_val.category != \"Religion\" ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check\n",
      "(370, 9)\n",
      "(1222, 9)\n",
      "(1592, 9)\n",
      "validation set\n",
      "(93, 9)\n",
      "(306, 9)\n",
      "(399, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity check\")\n",
    "print(df_train_religious.shape)\n",
    "print(df_train_other.shape)\n",
    "print(df_train.shape)\n",
    "print(\"validation set\")\n",
    "print(df_val_religious.shape)\n",
    "print(df_val_other.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370, 607)\n",
      "(93, 607)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_religion.shape)\n",
    "print(X_val_religion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro , con shape are (370, 840554) (370, 840554)\n",
      "pro , con shape are (93, 840554) (93, 840554)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.1, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841057) (370, 841057)\n",
      "pro , con shape are (93, 841057) (93, 841057)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841222) (370, 841222)\n",
      "pro , con shape are (93, 841222) (93, 841222)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841289) (370, 841289)\n",
      "pro , con shape are (93, 841289) (93, 841289)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841325) (370, 841325)\n",
      "pro , con shape are (93, 841325) (93, 841325)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841343) (370, 841343)\n",
      "pro , con shape are (93, 841343) (93, 841343)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841354) (370, 841354)\n",
      "pro , con shape are (93, 841354) (93, 841354)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841358) (370, 841358)\n",
      "pro , con shape are (93, 841358) (93, 841358)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 841359) (370, 841359)\n",
      "pro , con shape are (93, 841359) (93, 841359)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "pro , con shape are (370, 518) (370, 518)\n",
      "pro , con shape are (93, 518) (93, 518)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.1, accuracy: 0.5698924731182796\n",
      "pro , con shape are (370, 683) (370, 683)\n",
      "pro , con shape are (93, 683) (93, 683)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.1, accuracy: 0.5806451612903226\n",
      "pro , con shape are (370, 750) (370, 750)\n",
      "pro , con shape are (93, 750) (93, 750)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.1, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 786) (370, 786)\n",
      "pro , con shape are (93, 786) (93, 786)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.1, accuracy: 0.6236559139784946\n",
      "pro , con shape are (370, 804) (370, 804)\n",
      "pro , con shape are (93, 804) (93, 804)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.1, accuracy: 0.6129032258064516\n",
      "pro , con shape are (370, 815) (370, 815)\n",
      "pro , con shape are (93, 815) (93, 815)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.1, accuracy: 0.6129032258064516\n",
      "pro , con shape are (370, 819) (370, 819)\n",
      "pro , con shape are (93, 819) (93, 819)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.1, accuracy: 0.6344086021505376\n",
      "pro , con shape are (370, 820) (370, 820)\n",
      "pro , con shape are (93, 820) (93, 820)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.1, accuracy: 0.6451612903225806\n",
      "pro , con shape are (370, 167) (370, 167)\n",
      "pro , con shape are (93, 167) (93, 167)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.2, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 234) (370, 234)\n",
      "pro , con shape are (93, 234) (93, 234)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.2, accuracy: 0.6559139784946236\n",
      "pro , con shape are (370, 270) (370, 270)\n",
      "pro , con shape are (93, 270) (93, 270)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.2, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 288) (370, 288)\n",
      "pro , con shape are (93, 288) (93, 288)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.2, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 299) (370, 299)\n",
      "pro , con shape are (93, 299) (93, 299)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.2, accuracy: 0.6774193548387096\n",
      "pro , con shape are (370, 303) (370, 303)\n",
      "pro , con shape are (93, 303) (93, 303)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.2, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 304) (370, 304)\n",
      "pro , con shape are (93, 304) (93, 304)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.2, accuracy: 0.6559139784946236\n",
      "pro , con shape are (370, 67) (370, 67)\n",
      "pro , con shape are (93, 67) (93, 67)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.30000000000000004, accuracy: 0.6129032258064516\n",
      "pro , con shape are (370, 103) (370, 103)\n",
      "pro , con shape are (93, 103) (93, 103)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.30000000000000004, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 121) (370, 121)\n",
      "pro , con shape are (93, 121) (93, 121)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.30000000000000004, accuracy: 0.5806451612903226\n",
      "pro , con shape are (370, 132) (370, 132)\n",
      "pro , con shape are (93, 132) (93, 132)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.30000000000000004, accuracy: 0.6236559139784946\n",
      "pro , con shape are (370, 136) (370, 136)\n",
      "pro , con shape are (93, 136) (93, 136)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.30000000000000004, accuracy: 0.6129032258064516\n",
      "pro , con shape are (370, 137) (370, 137)\n",
      "pro , con shape are (93, 137) (93, 137)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.30000000000000004, accuracy: 0.6129032258064516\n",
      "pro , con shape are (370, 36) (370, 36)\n",
      "pro , con shape are (93, 36) (93, 36)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.4, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 54) (370, 54)\n",
      "pro , con shape are (93, 54) (93, 54)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.4, accuracy: 0.6451612903225806\n",
      "pro , con shape are (370, 65) (370, 65)\n",
      "pro , con shape are (93, 65) (93, 65)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.4, accuracy: 0.6559139784946236\n",
      "pro , con shape are (370, 69) (370, 69)\n",
      "pro , con shape are (93, 69) (93, 69)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.4, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 70) (370, 70)\n",
      "pro , con shape are (93, 70) (93, 70)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.4, accuracy: 0.6559139784946236\n",
      "pro , con shape are (370, 18) (370, 18)\n",
      "pro , con shape are (93, 18) (93, 18)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.5, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 29) (370, 29)\n",
      "pro , con shape are (93, 29) (93, 29)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.5, accuracy: 0.6666666666666666\n",
      "pro , con shape are (370, 33) (370, 33)\n",
      "pro , con shape are (93, 33) (93, 33)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.5, accuracy: 0.6989247311827957\n",
      "pro , con shape are (370, 34) (370, 34)\n",
      "pro , con shape are (93, 34) (93, 34)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.5, accuracy: 0.6881720430107527\n",
      "pro , con shape are (370, 11) (370, 11)\n",
      "pro , con shape are (93, 11) (93, 11)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.6000000000000001, accuracy: 0.5913978494623656\n",
      "pro , con shape are (370, 15) (370, 15)\n",
      "pro , con shape are (93, 15) (93, 15)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.6000000000000001, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 16) (370, 16)\n",
      "pro , con shape are (93, 16) (93, 16)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.6000000000000001, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 4) (370, 4)\n",
      "pro , con shape are (93, 4) (93, 4)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.7000000000000001, accuracy: 0.6021505376344086\n",
      "pro , con shape are (370, 5) (370, 5)\n",
      "pro , con shape are (93, 5) (93, 5)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.7000000000000001, accuracy: 0.5806451612903226\n",
      "pro , con shape are (370, 1) (370, 1)\n",
      "pro , con shape are (93, 1) (93, 1)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.8, accuracy: 0.5053763440860215\n",
      "************ best min_df, best max_df, acc 0.5 0.8 0.6989247311827957\n",
      "pro , con shape are (1222, 2228111) (1222, 2228111)\n",
      "pro , con shape are (306, 2228111) (306, 2228111)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.1, min_df: 0.0, accuracy: 0.5882352941176471\n",
      "pro , con shape are (1222, 2228508) (1222, 2228508)\n",
      "pro , con shape are (306, 2228508) (306, 2228508)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.0, accuracy: 0.6111111111111112\n",
      "pro , con shape are (1222, 2228606) (1222, 2228606)\n",
      "pro , con shape are (306, 2228606) (306, 2228606)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.0, accuracy: 0.6274509803921569\n",
      "pro , con shape are (1222, 2228645) (1222, 2228645)\n",
      "pro , con shape are (306, 2228645) (306, 2228645)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.0, accuracy: 0.6339869281045751\n",
      "pro , con shape are (1222, 2228662) (1222, 2228662)\n",
      "pro , con shape are (306, 2228662) (306, 2228662)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.0, accuracy: 0.6535947712418301\n",
      "pro , con shape are (1222, 2228672) (1222, 2228672)\n",
      "pro , con shape are (306, 2228672) (306, 2228672)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.0, accuracy: 0.6633986928104575\n",
      "pro , con shape are (1222, 2228676) (1222, 2228676)\n",
      "pro , con shape are (306, 2228676) (306, 2228676)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.0, accuracy: 0.6633986928104575\n",
      "pro , con shape are (1222, 2228677) (1222, 2228677)\n",
      "pro , con shape are (306, 2228677) (306, 2228677)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.0, accuracy: 0.6633986928104575\n",
      "pro , con shape are (1222, 2228677) (1222, 2228677)\n",
      "pro , con shape are (306, 2228677) (306, 2228677)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.0, accuracy: 0.6633986928104575\n",
      "pro , con shape are (1222, 397) (1222, 397)\n",
      "pro , con shape are (306, 397) (306, 397)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.1, accuracy: 0.7156862745098039\n",
      "pro , con shape are (1222, 495) (1222, 495)\n",
      "pro , con shape are (306, 495) (306, 495)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.1, accuracy: 0.7320261437908496\n",
      "pro , con shape are (1222, 534) (1222, 534)\n",
      "pro , con shape are (306, 534) (306, 534)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.1, accuracy: 0.7189542483660131\n",
      "pro , con shape are (1222, 551) (1222, 551)\n",
      "pro , con shape are (306, 551) (306, 551)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.1, accuracy: 0.7189542483660131\n",
      "pro , con shape are (1222, 561) (1222, 561)\n",
      "pro , con shape are (306, 561) (306, 561)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.1, accuracy: 0.7549019607843137\n",
      "pro , con shape are (1222, 565) (1222, 565)\n",
      "pro , con shape are (306, 565) (306, 565)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.1, accuracy: 0.7418300653594772\n",
      "pro , con shape are (1222, 566) (1222, 566)\n",
      "pro , con shape are (306, 566) (306, 566)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.1, accuracy: 0.7450980392156863\n",
      "pro , con shape are (1222, 566) (1222, 566)\n",
      "pro , con shape are (306, 566) (306, 566)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.1, accuracy: 0.7450980392156863\n",
      "pro , con shape are (1222, 99) (1222, 99)\n",
      "pro , con shape are (306, 99) (306, 99)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.2, accuracy: 0.7320261437908496\n",
      "pro , con shape are (1222, 138) (1222, 138)\n",
      "pro , con shape are (306, 138) (306, 138)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.2, accuracy: 0.738562091503268\n",
      "pro , con shape are (1222, 155) (1222, 155)\n",
      "pro , con shape are (306, 155) (306, 155)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.2, accuracy: 0.7418300653594772\n",
      "pro , con shape are (1222, 165) (1222, 165)\n",
      "pro , con shape are (306, 165) (306, 165)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.2, accuracy: 0.7450980392156863\n",
      "pro , con shape are (1222, 169) (1222, 169)\n",
      "pro , con shape are (306, 169) (306, 169)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.2, accuracy: 0.7418300653594772\n",
      "pro , con shape are (1222, 170) (1222, 170)\n",
      "pro , con shape are (306, 170) (306, 170)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.2, accuracy: 0.7516339869281046\n",
      "pro , con shape are (1222, 170) (1222, 170)\n",
      "pro , con shape are (306, 170) (306, 170)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.2, accuracy: 0.7516339869281046\n",
      "pro , con shape are (1222, 39) (1222, 39)\n",
      "pro , con shape are (306, 39) (306, 39)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.30000000000000004, accuracy: 0.7222222222222222\n",
      "pro , con shape are (1222, 56) (1222, 56)\n",
      "pro , con shape are (306, 56) (306, 56)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.30000000000000004, accuracy: 0.738562091503268\n",
      "pro , con shape are (1222, 66) (1222, 66)\n",
      "pro , con shape are (306, 66) (306, 66)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.30000000000000004, accuracy: 0.7647058823529411\n",
      "pro , con shape are (1222, 70) (1222, 70)\n",
      "pro , con shape are (306, 70) (306, 70)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.30000000000000004, accuracy: 0.7549019607843137\n",
      "pro , con shape are (1222, 71) (1222, 71)\n",
      "pro , con shape are (306, 71) (306, 71)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.30000000000000004, accuracy: 0.761437908496732\n",
      "pro , con shape are (1222, 71) (1222, 71)\n",
      "pro , con shape are (306, 71) (306, 71)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.30000000000000004, accuracy: 0.761437908496732\n",
      "pro , con shape are (1222, 17) (1222, 17)\n",
      "pro , con shape are (306, 17) (306, 17)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.4, accuracy: 0.6830065359477124\n",
      "pro , con shape are (1222, 27) (1222, 27)\n",
      "pro , con shape are (306, 27) (306, 27)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.4, accuracy: 0.7418300653594772\n",
      "pro , con shape are (1222, 31) (1222, 31)\n",
      "pro , con shape are (306, 31) (306, 31)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.4, accuracy: 0.7352941176470589\n",
      "pro , con shape are (1222, 32) (1222, 32)\n",
      "pro , con shape are (306, 32) (306, 32)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.4, accuracy: 0.7450980392156863\n",
      "pro , con shape are (1222, 32) (1222, 32)\n",
      "pro , con shape are (306, 32) (306, 32)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.4, accuracy: 0.7450980392156863\n",
      "pro , con shape are (1222, 10) (1222, 10)\n",
      "pro , con shape are (306, 10) (306, 10)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.5, accuracy: 0.7352941176470589\n",
      "pro , con shape are (1222, 14) (1222, 14)\n",
      "pro , con shape are (306, 14) (306, 14)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.5, accuracy: 0.7320261437908496\n",
      "pro , con shape are (1222, 15) (1222, 15)\n",
      "pro , con shape are (306, 15) (306, 15)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.5, accuracy: 0.7222222222222222\n",
      "pro , con shape are (1222, 15) (1222, 15)\n",
      "pro , con shape are (306, 15) (306, 15)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.5, accuracy: 0.7222222222222222\n",
      "pro , con shape are (1222, 4) (1222, 4)\n",
      "pro , con shape are (306, 4) (306, 4)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.6000000000000001, accuracy: 0.6339869281045751\n",
      "pro , con shape are (1222, 5) (1222, 5)\n",
      "pro , con shape are (306, 5) (306, 5)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.6000000000000001, accuracy: 0.6764705882352942\n",
      "pro , con shape are (1222, 5) (1222, 5)\n",
      "pro , con shape are (306, 5) (306, 5)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.6000000000000001, accuracy: 0.6764705882352942\n",
      "pro , con shape are (1222, 1) (1222, 1)\n",
      "pro , con shape are (306, 1) (306, 1)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.7000000000000001, accuracy: 0.6535947712418301\n",
      "pro , con shape are (1222, 1) (1222, 1)\n",
      "pro , con shape are (306, 1) (306, 1)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.7000000000000001, accuracy: 0.6535947712418301\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-e5171e942c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msearch_max_df_min_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val_religion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearch_max_df_min_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_other\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val_other\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-bbafb6186b24>\u001b[0m in \u001b[0;36msearch_max_df_min_df\u001b[0;34m(df_train, df_val)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdocument_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_by_side\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_feature_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_feature_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m   1225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m   1093\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "search_max_df_min_df(df_train_religion, df_val_religion)\n",
    "search_max_df_min_df(df_train_other, df_val_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro , con shape are (370, 841358) (370, 841358)\n",
      "pro , con shape are (93, 841358) (93, 841358)\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.1, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.0, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.2, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.1, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.30000000000000004, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9000000000000001, min_df: 0.2, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.4, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.30000000000000004, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.5, min_df: 0.4, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6000000000000001, min_df: 0.4, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.4, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.4, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.4, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.6, min_df: 0.5, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7, min_df: 0.5, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.5, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.9, min_df: 0.5, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.7000000000000001, min_df: 0.6000000000000001, accuracy: 0.4946236559139785\n",
      "====================================\n",
      "Logistic Regression testing set report:\n",
      "max_df: 0.8, min_df: 0.6000000000000001, accuracy: 0.4946236559139785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-c06ef8e9c814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_religion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_feature_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_religion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_val_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_religion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_feature_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_religion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreport_religion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_max_df_min_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_religion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_religion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvectorizer_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-84f2027f84e5>\u001b[0m in \u001b[0;36msearch_max_df_min_df\u001b[0;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1416\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             )\n\u001b[1;32m    763\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 618\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/env_nlp/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up the vectorizer\n",
    "vectorizer_religion = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=0, stop_words='english', ngram_range=(1,3))\n",
    "document_train_religion = get_text_by_side(df_train_religion)\n",
    "vectorizer_religion.fit(document_train_religion)\n",
    "X_train_religion, y_train_religion = get_all_feature_label(df_train_religion, vectorizer_religion)\n",
    "X_val_religion, y_val_religion = get_all_feature_label(df_val_religion, vectorizer_religion)\n",
    "report_religion = search_max_df_min_df(X_train_religion, y_train_religion, X_val_religion, y_val_religion)\n",
    "\n",
    "vectorizer_other = TfidfVectorizer(sublinear_tf=True, max_df=0.8, min_df=0, stop_words='english', ngram_range=(1,3))\n",
    "document_train_other = get_text_by_side(df_train_other)\n",
    "vectorizer_other.fit(document_train_other)\n",
    "X_train_other, y_train_other = get_all_feature_label(df_train_other, vectorizer_other)\n",
    "X_val_other, y_val_other = get_all_feature_label(df_val_other, vectorizer_other)\n",
    "report_other = search_max_df_min_df(X_train_other, y_train_other, X_val_other, y_val_other)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78e94ca1a29e9011c2866c841de859bf08fcc5b57f07b9ffb161018ea406f8e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
